{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning thêm data sentence bert dùng word2vec để tính similarity of sentence\n",
    "\n",
    "## Các bước thực hiện:\n",
    "\n",
    "1. Load data từ database, dùng underthesea để tách câu.\n",
    "2. Dùng word2vec để tính vector của câu.\n",
    "3. Training data cho sentence bert dùng vector sinh ra ở [2].\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'regress_adapter' from '/Users/ngocp/Documents/projects/pyml/botapp/research/../algorithm/regress_adapter.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import _stop_words\n",
    "import string\n",
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import gzip\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# how to import module from ../alorithm/\n",
    "import sys\n",
    "sys.path.append('../algorithm')\n",
    "from importlib import reload\n",
    "import helper\n",
    "import regress_adapter\n",
    "reload(helper)\n",
    "reload(regress_adapter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngocp/Documents/projects/pyml/botapp/research/../algorithm/regress_adapter.py:28: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  close      date_y\n",
      "3790  VNDIRECT: Hầu hết giá cổ phiếu của dệt may hiệ...  27680  2022-01-04\n",
      "3789  Cách tốt nhất để các nhà đầu tư bảo vệ mình kh...  27680  2022-01-04\n",
      "3788  Đảng uỷ cơ quan Uỷ ban Chứng khoán Nhà nước: S...  27680  2022-01-04\n",
      "3787  Truyền hình K+ tặng đầu thu miễn phí đón ‘Tết ...  27680  2022-01-04\n",
      "3786  Licogi 14 (L14) “bơm” thêm 214 tỷ đồng đầu tư ...  27190  2022-01-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = regress_adapter.get_data('TPB', '2022-01-01', '2023-09-29')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we build model word2vec to caculate similarity of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VNDIRECT: Hầu hết giá cổ phiếu của dệt may hiện đang gần với giá trị hợp lý, động lực tăng trưởng đến từ nhiều dự án khu công nghiệp\\n\\n\\n\\n\\r\\n                    TNG&MSH&STK&ADS&GIL&TCM&VGT: \\n\\n\\n\\n\\nGiá hiện tại\\n\\n\\n\\n\\nThay đổi\\n\\n\\n\\n\\n\\n\\n\\nXem hồ sơ doanh nghiệp \\n\\n\\n\\n\\nTIN MỚI\\n\\n\\n\\n\\nNhiều mã cổ phiếu dệt may tăng mạnhTheo phân tích mới đây từ Trung tâm Thông tin công nghiệp và thương mại (Bộ Công thương), năm 2021, ngành dệt may Việt Nam đã về đích với 39 tỷ USD kim ngạch xuất khẩu, tăng 11,2% so với năm 2020, tương đương với thời điểm trước dịch (năm 2019). Nhiều doanh nghiệp dệt may có KQKD tăng trưởng ấn tượng: Theo ước tính của VNDIRECT, tổng doanh thu Q4/21 của các công ty dệt may niêm yết tăng 24,1% svck, trong khi LN ròng của ngành tăng 57,0% svck trong Q4/21, cao hơn 82,0% so với Q3/21. Các doanh nghiệp có mức tăng trưởng LN ấn tượng phải kể đến ADS (+ 303,2% svck), VGT (+ 161,4% svck) và STK (+ 92,9% svck).So với thời điểm dịch Covid-19 lần thứ tư bùng phát (tháng 7/2021), nhiều mã cổ phiếu dệt may bật tăng mạnh, TNG (+ 131,76%), EVE (+62,62%), MSH (+64,62%), VGT (+80,85%), ADS (+76,1%), STK (+66,15%).Mức tăng trưởng này đến từ sự phục hồi trong Quý IV/2021, sau thời gian dài ảnh hưởng bởi giãn cách xã hội trong Quý III/2021. Xuất khẩu vải và hàng may mặc trong Quý IV/2021 tăng 21,6% svck lên 9,5 tỷ USD.\\xa0Nguồn: MOIT, VNDIRECT ResearchHưởng lợi từ chiến tranh thương mại Mỹ-TrungViệt Nam còn được hưởng lợi từ việc dịch chuyển các đơn hàng vải, may mặc và xơ sợi từ Trung Quốc do chiến tranh thương mại Mỹ-Trung. Tháng 12/2021, tổng thống Joe Biden ký sắc lệnh cấm nhập khẩu sợi từ Tân Cương (Trung Quốc), VNDIRECT kỳ vọng các doanh nghiệp sản xuất sợi như ADS, STK,VGT được hưởng lợi từ miếng bánh Tân Cương. Đặc biệt, trong năm 2021, Việt Nam đã vượt qua Hàn Quốc, trở thành nước xuất khẩu xơ sợi lớn thứ 6 trên thế giới, với tổng giá trị xuất khẩu sợi đạt 5,6 tỷ USD vào năm 2021 (+ 50,8% svck).Nguồn: MOIT, VNDIRECT ResearchThị trường mỹ và EU phục hồiHiện tại, Mỹ vẫn là thị trường xuất khẩu lớn nhất của ngành dệt may Việt Nam. Theo thống kê từ Tổng cục Hải quan, xuất khẩu hàng dệt may của Việt Nam sang thị trường Mỹ tháng 01/2022 đạt 1,85 tỷ USD, tăng 2% so với tháng 12/2021 và tăng 42% so với tháng 01/2021.Theo Liên đoàn Dệt may Châu Âu (Euratex), ngành dệt may EU tiếp tục chứng kiến sự phục hồi sau COVID-19. Cụ thể, giá trị sản lượng dệt may đã trở lại mức trước đại dịch vào cuối T11/ 2021.Thời điểm hiện tại, nhiều doanh nghiệp đã nhận được các đơn hàng đến quý III/2022, tỷ lệ tiêm chủng vaccine của cả nước đạt hơn 81% giúp các doanh nghiệp có thể đảm bảo nguồn cung lao động, hoạt động tối đa công suất.Thị trường xuất khẩu ngành dệt may Việt NamVNDIRECT kỳ vọng cho thuê bất động sản KCN sẽ là động lực tăng trưởng doanh thu chính của một số công ty dệt may trong năm 2022Các doanh nghiệp dệt may như GIL, ADS, TCM, TNG đã mở rộng kinh doanh sang lĩnh vực bất động sản (BĐS) và BĐS khu công nghiệp. VNDIRECT kỳ vọng mảng kinh doanh mới sẽ hỗ trợ các doanh nghiệp duy trì tăng trưởng lợi nhuận trong giai đoạn 2022-25.VNDIRECT cho rằng hầu hết giá cổ phiếu của dệt may hiện đang gần với giá trị hợp lý và nhà đầu tư nên có chọn lọc, tập trung đầu tư vào các doanh nghiệp đầu ngành và có kế hoạch mở rộng công suất nhà máy trong 2022-25.VNDIRECT lựa chọn 2 doanh nghiệp STK và MSH vì tiềm năng tăng trưởng từ các dự án Unitex và SH10. Cụ thể, VNDIRECT kỳ vọng lợi nhuận STK sẽ tăng 90,4% svck trong năm 2021 và đạt CAGR 37,0% trong giai đoạn 2021-23. Phân khúc sợi tái chế trong năm 2022 sẽ đặc biệt hưởng lợi từ sự phục hồi nhu cầu của thị trường nội địa và tình hình thiếu điện ở Trung Quốc. Nhà máy Unitex giai đoạn 1 sẽ đi vào hoạt động thương mại trong Q1/23 nâng tổng sản lượng tiêu thụ trong năm dự kiến đạt 76.800 tấn/năm để đáp ứng nhu cầu ngày càng tăng đối với sợi tái chế và sợi nguyên sinh.Đối với MSH, VNDIRECT kỳ vọng doanh nghiệp sẽ tiếp tục duy trì triển vọng tích cực trong 2022-2023 nhờ lượng đơn đặt hàng từ các khách hàng Mỹ tăng trở lại và nhà máy SH10 sẽ giúp doanh thu FOB tăng trưởng 15%/20% svck trong 2022/23. MSH đã bán các khoản phải thu từ New York & Company với giá trị thu hồi là 80 tỷ đồng.BSC: 2022 sẽ là năm của những doanh nghiệp Dệt may tăng trưởng cả mảng kinh doanh cốt lõi và \"lấn sân\" đầu tư bất động sản\\nhttps://cafef.vn/vndirect-hau-het-gia-co-phieu-cua-det-may-hien-dang-gan-voi-gia-tri-hop-ly-dong-luc-tang-truong-den-tu-nhieu-du-an-khu-cong-nghiep-20220331123159622.chn\\n\\n\\n', 'Cách tốt nhất để các nhà đầu tư bảo vệ mình khỏi hành vi thao túng thị trường là gì?\\n\\n\\n\\n\\nTIN MỚI\\n\\n\\n\\n\\nTheo Seeking Alpha (Mỹ), các nhà đầu tư cần bình tĩnh và nên đầu tư dài hạn để tránh được rủi ro do thao túng thị trường chứng khoán gây ra.Bên cạnh đó, theo Tập đoàn dịch vụ tài chính đa quốc gia Nasdaq (Mỹ), cách tốt nhất để các nhà đầu tư bảo vệ mình khỏi thao túng thị trường chứng khoán đó là đầu tư dài hạn. Các nhà đầu tư cần hiểu được các kiểu thao túng thị trường để từ đó đưa ra quyết định đầu tư tốt hơn.Theo Investopedia, có 5 cách để thao túng thị trường chứng khoán. Trên thực tế, thao túng đang diễn ra tràn lan trên thị trường chứng khoán ngày nay. Hiểu được cách thức thao túng thị trường sẽ giúp cho nhà đầu tư có lợi thế hơn và kiếm được lời nhiều hơn.Fake News (tin tức giả)Đây là phương thức truyền bá thông tin sai lệch, gây hiểu lầm về một công ty. Thông thường, thông tin giả được sử dụng bởi các nhà đầu tư trên thị trường chứng khoán lâu năm có độ am hiểu về phương tiện truyền thông. Một số nhà đầu tư cố gắng truyền bá những tin tức giả mạo về một công ty hoặc thậm chí toàn bộ thị trường để làm thị trường đi theo hướng có lợi cho mình.Chính vì thế, các nhà đầu tư cá nhân cần phải xác minh kỹ nguồn thông tin trước khi quyết định là cách tốt nhất để tránh khỏi sự thao túng này. Theo Nasdaq, có một cách có thể kiếm lời từ những thông tin giả đó là đợi cổ phiếu tăng đột biến cao hơn hoặc thấp hơn dựa trên những tin tức giả, sau đó tham gia giao dịch theo hướng ngược lại.Pump And Dump (bơm và xả)Đây là phương thức dùng để mua một lượng cổ phiếu lớn khiến giá và lượng tăng đột biến. Nhờ đó, giá cổ phiếu bị thổi lên vượt hơn rất nhiều giá trị thực, từ đó tạo tín hiệu khiến nhiều nhà đầu tư mua vào. Đến một thời điểm nhất định, người thực hiện bơm giá sẽ bán phần lớn hoặc toàn bộ cổ phiếu của mình để thu lợi. Sau đó, giá cổ phiếu sẽ lao dốc và khi các nhà đầu tư phát hiện ra thì đã muộn, những người mua sau là những người gánh thiệt hại nhiều nhất.Cách để bảo vệ bản thân khỏi những đợt bơm giá và bán phá giá đó là tránh mua các cổ phiếu đang tăng giá quá cao hơn mức bình thường. Trên thực tế, các nhà đầu tư nhỏ có thể kiếm được lời từ việc bơm giá bằng chiến lược thực hiện các giao dịch nghịch xu hướng thịnh hành để kiếm lợi nhuận.Spoofing The Tape (tạo lệnh mua giả)Đây là hình thức mà những người thao túng thị trường sẽ đặt mua một lượng lớn cổ phiếu nhưng lại không có ý định mua thật sự. Từ việc tạo lệnh mua giả, người thao túng tạo ra một cái bẫy khiến các nhà đầu tư khác tưởng chừng như có \"miếng mồi ngon\" và mua/bán theo.Tuy nhiên, sau khi có một lượng lớn người đầu tư mua vào thì những người thao túng thị trường này sẽ rút lệnh mua trước vài giây giao dịch. Sau khi người thao túng kéo lệnh về, thị trường sẽ giảm xuống. Cuối cùng, các nhà đầu tư khác sẽ chịu thua lỗ sau khi rơi vào bẫy.Do đó, theo Investopedia, cách để nhà đầu tư bảo vệ bản thân trong trường hợp này đó là tránh đầu tư ngắn hạn. Bên cạnh đó, theo Nasdaq, những nhà đầu tư tham gia vào thị trường chứng khoán lâu năm sẽ nhận biết được cách thao túng thị tường theo kiểu tạo lệnh giả. Nếu nhà đầu tư thật sự sành sỏi thì có thể kiếm được lợi nhuận mặc dù thị trường đang bị thao túng do tạo lệnh mua giả.Wash Trading (mua và bán cùng một loại mã liên tục và ngay lập tức)Hình thức này diễn ra khi nhà đầu tư lớn thực hiện việc mua và bán cùng một loại mã liên tục và gần như ngay lập tức. Việc mua và bán nhanh chóng làm tăng khối lượng, thu hút các nhà đầu tư bởi khối lượng tăng vọt. Do đó, sẽ có nhiều nhà đầu tư ngắn hạn bị lừa và tạo ra lợi ích cho những người đang thao túng thị trường.Theo Nasdaq, thao túng thị trường bằng việc mua và bán cùng một loại mã liên tục và gần như ngay lập tức sẽ không làm ảnh hưởng đến các nhà đầu tư dài hạn. Chính vì vậy, cách tốt nhất trong trường hợp này vẫn là tập trung đầu tư dài hạn hơn là ngắn hạn.Bear Raiding (sự săn lùng giá xuống)Trong trường hợp này, các nhà đầu tư cấu kết với nhau để đẩy giá cổ phiếu xuống thấp hơn thông qua việc bán khống và lan truyền những tin đồn bất lợi về công ty. Mục đích của phương thức này vẫn là tạo ra một cái bẫy để dụ những nhà đầu tư chưa hiểu rõ các chiêu trò trong thao túng thị trường.Ngoài ra, hành động thao túng sẽ gây hoang mang cho những người lạc quan về thị trường. Từ đó, người thao túng tạo áp lực buộc các nhà đầu tư cá nhân bán tống bán tháo quá mức cho phép để người thao túng có thể thanh lý các hợp đồng bán khống để kiếm lời.Theo Investopedia, trong trường hợp này nếu nhà đầu tư nắm được cách thức hoạt động của thị trường thì hoàn toàn có thể tránh được rủi ro và sinh lời.\"Các vụ Nguyễn Phương Hằng, thao túng chứng khoán có tính chất thách thức pháp luật\"\\nhttps://cafef.vn/cach-tot-nhat-de-cac-nha-dau-tu-bao-ve-minh-khoi-hanh-vi-thao-tung-thi-truong-la-gi-20220401091951827.chn\\n\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "data_list = data['text'].values.tolist()\n",
    "print(data_list[:2])\n",
    "\n",
    "def split_word_token(texts = []):\n",
    "    clear_ld = lambda t: helper.NLP(t).get_words_feature()\n",
    "    t = map(clear_ld, texts)\n",
    "    return list(t)\n",
    "\n",
    "data_token = split_word_token(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../cached/sBertTrain-word2vec_new.model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/gensim/utils.py:764\u001b[0m, in \u001b[0;36mSaveLoad.save\u001b[0;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 764\u001b[0m     _pickle\u001b[39m.\u001b[39;49mdump(\u001b[39mself\u001b[39;49m, fname_or_handle, protocol\u001b[39m=\u001b[39;49mpickle_protocol)\n\u001b[1;32m    765\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39msaved \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m object\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: file must have a 'write' attribute",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ngocp/Documents/projects/pyml/botapp/research/sentence-bert-train-word2vec.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ngocp/Documents/projects/pyml/botapp/research/sentence-bert-train-word2vec.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Word2Vec\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ngocp/Documents/projects/pyml/botapp/research/sentence-bert-train-word2vec.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m Word2Vec(data_token, vector_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, epochs\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, workers\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ngocp/Documents/projects/pyml/botapp/research/sentence-bert-train-word2vec.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39;49msave(\u001b[39m'\u001b[39;49m\u001b[39m../cached/sBertTrain-word2vec_new.model\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/gensim/models/word2vec.py:1912\u001b[0m, in \u001b[0;36mWord2Vec.save\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1901\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1902\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Save the model.\u001b[39;00m\n\u001b[1;32m   1903\u001b[0m \u001b[39m    This saved model can be loaded again using :func:`~gensim.models.word2vec.Word2Vec.load`, which supports\u001b[39;00m\n\u001b[1;32m   1904\u001b[0m \u001b[39m    online training and getting vectors for vocabulary words.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1910\u001b[0m \n\u001b[1;32m   1911\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1912\u001b[0m     \u001b[39msuper\u001b[39;49m(Word2Vec, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49msave(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/gensim/utils.py:767\u001b[0m, in \u001b[0;36mSaveLoad.save\u001b[0;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[1;32m    765\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39msaved \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m object\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    766\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:  \u001b[39m# `fname_or_handle` does not have write attribute\u001b[39;00m\n\u001b[0;32m--> 767\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_smart_save(fname_or_handle, separately, sep_limit, ignore, pickle_protocol\u001b[39m=\u001b[39;49mpickle_protocol)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/gensim/utils.py:611\u001b[0m, in \u001b[0;36mSaveLoad._smart_save\u001b[0;34m(self, fname, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[1;32m    607\u001b[0m restores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_specials(\n\u001b[1;32m    608\u001b[0m     fname, separately, sep_limit, ignore, pickle_protocol, compress, subname,\n\u001b[1;32m    609\u001b[0m )\n\u001b[1;32m    610\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 611\u001b[0m     pickle(\u001b[39mself\u001b[39;49m, fname, protocol\u001b[39m=\u001b[39;49mpickle_protocol)\n\u001b[1;32m    612\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    613\u001b[0m     \u001b[39m# restore attribs handled specially\u001b[39;00m\n\u001b[1;32m    614\u001b[0m     \u001b[39mfor\u001b[39;00m obj, asides \u001b[39min\u001b[39;00m restores:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/gensim/utils.py:1442\u001b[0m, in \u001b[0;36mpickle\u001b[0;34m(obj, fname, protocol)\u001b[0m\n\u001b[1;32m   1429\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpickle\u001b[39m(obj, fname, protocol\u001b[39m=\u001b[39mPICKLE_PROTOCOL):\n\u001b[1;32m   1430\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Pickle object `obj` to file `fname`, using smart_open so that `fname` can be on S3, HDFS, compressed etc.\u001b[39;00m\n\u001b[1;32m   1431\u001b[0m \n\u001b[1;32m   1432\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \n\u001b[1;32m   1441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(fname, \u001b[39m'\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m fout:  \u001b[39m# 'b' for binary, needed on Windows\u001b[39;00m\n\u001b[1;32m   1443\u001b[0m         _pickle\u001b[39m.\u001b[39mdump(obj, fout, protocol\u001b[39m=\u001b[39mprotocol)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/smart_open/smart_open_lib.py:177\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39mif\u001b[39;00m transport_params \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     transport_params \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 177\u001b[0m fobj \u001b[39m=\u001b[39m _shortcut_open(\n\u001b[1;32m    178\u001b[0m     uri,\n\u001b[1;32m    179\u001b[0m     mode,\n\u001b[1;32m    180\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    181\u001b[0m     buffering\u001b[39m=\u001b[39;49mbuffering,\n\u001b[1;32m    182\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m    183\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    184\u001b[0m     newline\u001b[39m=\u001b[39;49mnewline,\n\u001b[1;32m    185\u001b[0m )\n\u001b[1;32m    186\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[39mreturn\u001b[39;00m fobj\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/smart_open/smart_open_lib.py:363\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39mif\u001b[39;00m errors \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m    361\u001b[0m     open_kwargs[\u001b[39m'\u001b[39m\u001b[39merrors\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m errors\n\u001b[0;32m--> 363\u001b[0m \u001b[39mreturn\u001b[39;00m _builtin_open(local_path, mode, buffering\u001b[39m=\u001b[39;49mbuffering, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mopen_kwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../cached/sBertTrain-word2vec_new.model'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(data_token, vector_size=100, epochs=100, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./cached/sBertTrain-word2vec_new.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sents(texts = []):\n",
    "    def split_sentences(text):\n",
    "        try:\n",
    "            sents = helper.NLP(text).split_sentences()\n",
    "        except Exception as e:\n",
    "            print('Error:', e, text)\n",
    "            sents = []\n",
    "        sents_1 = [sent for sent in sents if sent.strip() != \"\"]\n",
    "        return sents_1\n",
    "\n",
    "    t = map(split_sentences, texts)\n",
    "    return list(t)\n",
    "\n",
    "def flatten_list(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: expected string or bytes-like object nan\n",
      "Error: expected string or bytes-like object nan\n",
      "Error: expected string or bytes-like object nan\n",
      "Error: expected string or bytes-like object nan\n",
      "Error: expected string or bytes-like object nan\n",
      "Error: expected string or bytes-like object nan\n",
      "Error: expected string or bytes-like object nan\n",
      "Error: expected string or bytes-like object nan\n",
      "Error: expected string or bytes-like object nan\n",
      "Error: expected string or bytes-like object nan\n",
      "Error: expected string or bytes-like object nan\n"
     ]
    }
   ],
   "source": [
    "sents = split_sents(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5085\n"
     ]
    }
   ],
   "source": [
    "print(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent: VNDIRECT: Hầu hết giá cổ phiếu của dệt may hiện đang gần với giá trị hợp lý, động lực tăng trưởng đến từ nhiều dự án khu công nghiệp\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                    TNG&MSH&STK&ADS&GIL&TCM&VGT: \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Giá hiện tại\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Thay đổi\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Xem hồ sơ doanh nghiệp \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "TIN MỚI\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Nhiều mã cổ phiếu dệt may tăng mạnhTheo phân tích mới đây từ Trung tâm Thông tin công nghiệp và thương mại (Bộ Công thương), năm 2021, ngành dệt may Việt Nam đã về đích với 39 tỷ USD kim ngạch xuất khẩu, tăng 11,2% so với năm 2020, tương đương với thời điểm trước dịch (năm 2019).\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "# Calculate sentence embeddings by averaging word embeddings\n",
    "def calculate_sentence_embedding(embeddings):\n",
    "    if len(embeddings) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "def get_embed_sent(sent):\n",
    "    sent_words = split_word_token([sent])\n",
    "    question_embeddings = [model.wv[word] for word in sent_words[0] if word in model.wv]\n",
    "    question_embedding = calculate_sentence_embedding(question_embeddings)\n",
    "    return question_embedding\n",
    "\n",
    "print('sent:', sents[0][0])\n",
    "t = get_embed_sent(sents[0][0])\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentencesDataset, InputExample, losses\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "inputExpTrain = []\n",
    "for post in sents:\n",
    "    for i, sentence in enumerate(post):\n",
    "        if i == len(post) - 1:\n",
    "            break\n",
    "        sent1 = sentence\n",
    "        sent2 = post[i+1]\n",
    "        label = cosine_similarity([get_embed_sent(sent1)], [get_embed_sent(sent2)])\n",
    "        inputExpTrain.append([sentence, post[i+1], label[0][0]])\n",
    "       \n",
    "\n",
    "# construct dataframe from inputExpTrain\n",
    "df = pd.DataFrame(inputExpTrain, columns=['text1', 'text2', 'label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 109810 entries, 0 to 109809\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   text1   109810 non-null  object \n",
      " 1   text2   109810 non-null  object \n",
      " 2   label   109810 non-null  float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 2.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               text1  \\\n",
      "0  VNDIRECT: Hầu hết giá cổ phiếu của dệt may hiệ...   \n",
      "1  Nhiều doanh nghiệp dệt may có KQKD tăng trưởng...   \n",
      "2  Các doanh nghiệp có mức tăng trưởng LN ...   \n",
      "3  Xuất khẩu vải và hàng may mặc trong Quý IV/202...   \n",
      "4  Nguồn: MOIT, VNDIRECT ResearchHưởng lợi từ chi...   \n",
      "\n",
      "                                               text2     label  \n",
      "0  Nhiều doanh nghiệp dệt may có KQKD tăng trưởng...  0.802747  \n",
      "1  Các doanh nghiệp có mức tăng trưởng LN ...  0.763249  \n",
      "2  Xuất khẩu vải và hàng may mặc trong Quý IV/202...  0.761638  \n",
      "3  Nguồn: MOIT, VNDIRECT ResearchHưởng lợi từ chi...  0.674743  \n",
      "4  Tháng 12/2021, tổng thống Joe Biden ký sắc lện...  0.782528  \n"
     ]
    }
   ],
   "source": [
    "# how to min max scale for column label in df\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "df['label'] = min_max_scaler.fit_transform(df['label'].values.reshape(-1,1))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df into list of InputExample\n",
    "data_train = []\n",
    "for index, row in df.iterrows():\n",
    "    data_train.append(InputExample(texts=[row['text1'], row['text2']], label=row['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/sentences-train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fdc30311a4f4437a69c13ba2e5c381d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd79ef5ca024e0d9674f61509cc533d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/6864 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">12</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 </span>train_loss = losses.CosineSimilarityLoss(model)                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 #Tune the model</span>                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>12 model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">10</span>, warmup_steps=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">100</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">13 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/sentence_transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">SentenceT</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">ransformer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">722</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">fit</span>                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">719 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>skip_scheduler = scaler.get_scale() != scale_before_step           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">720 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">721 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>loss_value = loss_model(features, labels)                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>722 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>loss_value.backward()                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">723 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>torch.nn.utils.clip_grad_norm_(loss_model.parameters(), max_grad   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">724 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>optimizer.step()                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">725 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">487</span> in         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 484 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>create_graph=create_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 485 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>inputs=inputs,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 486 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 487 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch.autograd.backward(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 488 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=inputs                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 489 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 490 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">200</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">197 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># The reason we repeat same the comment below is that</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">198 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># some Python versions print out the first line of a multi-line function</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>200 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the bac</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">201 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>tensors, grad_tensors_, retain_graph, create_graph, inputs,                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">202 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>allow_unreachable=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to ru</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">203 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m12\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 9 \u001b[0mtrain_loss = losses.CosineSimilarityLoss(model)                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m10 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m11 \u001b[0m\u001b[2m#Tune the model\u001b[0m                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m12 model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=\u001b[94m10\u001b[0m, warmup_steps=\u001b[94m100\u001b[0m    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m13 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/sentence_transformers/\u001b[0m\u001b[1;33mSentenceT\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mransformer.py\u001b[0m:\u001b[94m722\u001b[0m in \u001b[92mfit\u001b[0m                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m719 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mskip_scheduler = scaler.get_scale() != scale_before_step           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m720 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m721 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mloss_value = loss_model(features, labels)                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m722 \u001b[2m│   │   │   │   │   │   \u001b[0mloss_value.backward()                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m723 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mtorch.nn.utils.clip_grad_norm_(loss_model.parameters(), max_grad   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m724 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0moptimizer.step()                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m725 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/\u001b[0m\u001b[1;33m_tensor.py\u001b[0m:\u001b[94m487\u001b[0m in         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mbackward\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 484 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcreate_graph=create_graph,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 485 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs=inputs,                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 486 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 487 \u001b[2m│   │   \u001b[0mtorch.autograd.backward(                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 488 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=inputs                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 489 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 490 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/Users/ngocp/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/autograd/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m200\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mbackward\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# The reason we repeat same the comment below is that\u001b[0m                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line function\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m200 \u001b[2m│   \u001b[0mVariable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to run the bac\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m│   │   \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m│   │   \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# Calls into the C++ engine to ru\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentencesDataset, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('nli-distilroberta-base-v2')\n",
    "\n",
    "#Define your train dataset, the dataloader and the train loss\n",
    "train_dataloader = DataLoader(data_train, shuffle=True, batch_size=16)\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "#Tune the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=10, warmup_steps=100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
